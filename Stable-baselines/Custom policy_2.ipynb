{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e8f9f0",
   "metadata": {},
   "source": [
    "# Custom LSTM Policy\n",
    "\n",
    "Source code: <https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/common/policies.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035c4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from stable_baselines.common.tf_util import batch_to_seq, seq_to_batch\n",
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "from stable_baselines.common.policies import RecurrentActorCriticPolicy, ActorCriticPolicy,register_policy, nature_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8cc50",
   "metadata": {},
   "source": [
    "## LSTM Policy 1\n",
    " + 1 Lstm Layer with 128 nodes (default: 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36496af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmPolicy1(RecurrentActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using LSTMs.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param n_lstm: (int) The number of LSTM cells (for recurrent policies)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) The size of the Neural network before the LSTM layer  (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture. Notation similar to the\n",
    "        format described in mlp_extractor but with additional support for a 'lstm' entry in the shared network part.\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param layer_norm: (bool) Whether or not to use layer normalizing LSTMs\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    recurrent = True\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm=128, reuse=False, layers=None,\n",
    "                 net_arch=None, act_fun=tf.tanh, cnn_extractor=nature_cnn, layer_norm=False, feature_extraction=\"mlp\",\n",
    "                 **kwargs):\n",
    "        # state_shape = [n_lstm * 2] dim because of the cell and hidden states of the LSTM\n",
    "        super(LstmPolicy1, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                                         state_shape=(2 * n_lstm, ), reuse=reuse,\n",
    "                                         scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if net_arch is None:  # Legacy mode\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            else:\n",
    "                warnings.warn(\"The layers parameter is deprecated. Use the net_arch parameter instead.\")\n",
    "\n",
    "            with tf.variable_scope(\"model\", reuse=reuse):\n",
    "                if feature_extraction == \"cnn\":\n",
    "                    extracted_features = cnn_extractor(self.processed_obs, **kwargs)\n",
    "                else:\n",
    "                    extracted_features = tf.layers.flatten(self.processed_obs)\n",
    "                    for i, layer_size in enumerate(layers):\n",
    "                        extracted_features = act_fun(linear(extracted_features, 'pi_fc' + str(i), n_hidden=layer_size,\n",
    "                                                            init_scale=np.sqrt(2)))\n",
    "                input_sequence = batch_to_seq(extracted_features, self.n_env, n_steps)\n",
    "                masks = batch_to_seq(self.dones_ph, self.n_env, n_steps)\n",
    "                rnn_output, self.snew = lstm(input_sequence, masks, self.states_ph, 'lstm1', n_hidden=n_lstm,\n",
    "                                             layer_norm=layer_norm)\n",
    "                rnn_output, self.snew = lstm(rnn_output, masks, self.snew, 'lstm2', n_hidden=n_lstm,\n",
    "                                             layer_norm=layer_norm)\n",
    "                rnn_output = seq_to_batch(rnn_output)\n",
    "                value_fn = linear(rnn_output, 'vf', 1)\n",
    "\n",
    "                self._proba_distribution, self._policy, self.q_value = \\\n",
    "                    self.pdtype.proba_distribution_from_latent(rnn_output, rnn_output)\n",
    "\n",
    "            self._value_fn = value_fn\n",
    "        else:  # Use the new net_arch parameter\n",
    "            if layers is not None:\n",
    "                warnings.warn(\"The new net_arch parameter overrides the deprecated layers parameter.\")\n",
    "            if feature_extraction == \"cnn\":\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            with tf.variable_scope(\"model\", reuse=reuse):\n",
    "                latent = tf.layers.flatten(self.processed_obs)\n",
    "                policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "                value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "\n",
    "                # Iterate through the shared layers and build the shared parts of the network\n",
    "                lstm_layer_constructed = False\n",
    "                for idx, layer in enumerate(net_arch):\n",
    "                    if isinstance(layer, int):  # Check that this is a shared layer\n",
    "                        layer_size = layer\n",
    "                        latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "                    elif layer == \"lstm\":\n",
    "                        if lstm_layer_constructed:\n",
    "                            raise ValueError(\"The net_arch parameter must only contain one occurrence of 'lstm'!\")\n",
    "                        input_sequence = batch_to_seq(latent, self.n_env, n_steps)\n",
    "                        masks = batch_to_seq(self.dones_ph, self.n_env, n_steps)\n",
    "                        rnn_output, self.snew = lstm(input_sequence, masks, self.states_ph, 'lstm1', n_hidden=n_lstm,\n",
    "                                                     layer_norm=layer_norm)\n",
    "                        latent = seq_to_batch(rnn_output)\n",
    "                        lstm_layer_constructed = True\n",
    "                    else:\n",
    "                        assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "                        if 'pi' in layer:\n",
    "                            assert isinstance(layer['pi'],\n",
    "                                              list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                            policy_only_layers = layer['pi']\n",
    "\n",
    "                        if 'vf' in layer:\n",
    "                            assert isinstance(layer['vf'],\n",
    "                                              list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                            value_only_layers = layer['vf']\n",
    "                        break  # From here on the network splits up in policy and value network\n",
    "\n",
    "                # Build the non-shared part of the policy-network\n",
    "                latent_policy = latent\n",
    "                for idx, pi_layer_size in enumerate(policy_only_layers):\n",
    "                    if pi_layer_size == \"lstm\":\n",
    "                        raise NotImplementedError(\"LSTMs are only supported in the shared part of the policy network.\")\n",
    "                    assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "                    latent_policy = act_fun(\n",
    "                        linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "\n",
    "                # Build the non-shared part of the value-network\n",
    "                latent_value = latent\n",
    "                for idx, vf_layer_size in enumerate(value_only_layers):\n",
    "                    if vf_layer_size == \"lstm\":\n",
    "                        raise NotImplementedError(\"LSTMs are only supported in the shared part of the value function \"\n",
    "                                                  \"network.\")\n",
    "                    assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "                    latent_value = act_fun(\n",
    "                        linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "\n",
    "                if not lstm_layer_constructed:\n",
    "                    raise ValueError(\"The net_arch parameter must contain at least one occurrence of 'lstm'!\")\n",
    "\n",
    "                self._value_fn = linear(latent_value, 'vf', 1)\n",
    "                # TODO: why not init_scale = 0.001 here like in the feedforward\n",
    "                self._proba_distribution, self._policy, self.q_value = \\\n",
    "                    self.pdtype.proba_distribution_from_latent(latent_policy, latent_value)\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            return self.sess.run([self.deterministic_action, self.value_flat, self.snew, self.neglogp],\n",
    "                                 {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})\n",
    "        else:\n",
    "            return self.sess.run([self.action, self.value_flat, self.snew, self.neglogp],\n",
    "                                 {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2debb00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\AppData\\Local\\Temp/ipykernel_20884/3040048196.py:44: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:160: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:194: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\anaconda3\\envs\\base_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:196: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "---------------------------------\n",
      "| explained_variance | -0.00161 |\n",
      "| fps                | 4        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 10.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.000573 |\n",
      "| fps                | 181      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 500      |\n",
      "| value_loss         | 6.05     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0014   |\n",
      "| fps                | 220      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 10.2     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0133  |\n",
      "| fps                | 244      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 6.02     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.00458  |\n",
      "| fps                | 257      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0077  |\n",
      "| fps                | 261      |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0107  |\n",
      "| fps                | 265      |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 3000     |\n",
      "| value_loss         | 5.96     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.128   |\n",
      "| fps                | 267      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 3.25     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.158   |\n",
      "| fps                | 272      |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 0.691    |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 3.59     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.42    |\n",
      "| fps                | 273      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.692    |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 19.9     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -2.81    |\n",
      "| fps                | 274      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.69     |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 34       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -2.71    |\n",
      "| fps                | 275      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.677    |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 263      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.052   |\n",
      "| fps                | 275      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.645    |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 4.1      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -2.95    |\n",
      "| fps                | 277      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 0.32     |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 4.81     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.911   |\n",
      "| fps                | 277      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 0.672    |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 851      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.87    |\n",
      "| fps                | 279      |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 0.663    |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 2.57     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0304  |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 0.65     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 2.05     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | -0.0128  |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 1.54     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.95    |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 0.482    |\n",
      "| total_timesteps    | 9000     |\n",
      "| value_loss         | 1.03e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.984   |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 0.614    |\n",
      "| total_timesteps    | 9500     |\n",
      "| value_loss         | 1.04     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.518   |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.3      |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 379      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.698   |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 0.29     |\n",
      "| total_timesteps    | 10500    |\n",
      "| value_loss         | 5.59e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.988   |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 2200     |\n",
      "| policy_entropy     | 0.538    |\n",
      "| total_timesteps    | 11000    |\n",
      "| value_loss         | 389      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -20.8    |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 2300     |\n",
      "| policy_entropy     | 0.686    |\n",
      "| total_timesteps    | 11500    |\n",
      "| value_loss         | 0.0742   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -13.8    |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 2400     |\n",
      "| policy_entropy     | 0.672    |\n",
      "| total_timesteps    | 12000    |\n",
      "| value_loss         | 0.00524  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.136   |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 2500     |\n",
      "| policy_entropy     | 0.0492   |\n",
      "| total_timesteps    | 12500    |\n",
      "| value_loss         | 9.97e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -291     |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 2600     |\n",
      "| policy_entropy     | 0.00663  |\n",
      "| total_timesteps    | 13000    |\n",
      "| value_loss         | 0.00256  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -3.97e+04 |\n",
      "| fps                | 284       |\n",
      "| nupdates           | 2700      |\n",
      "| policy_entropy     | 0.011     |\n",
      "| total_timesteps    | 13500     |\n",
      "| value_loss         | 102       |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.138    |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 0.0156   |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 503      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.00142  |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 0.0108   |\n",
      "| total_timesteps    | 14500    |\n",
      "| value_loss         | 200      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.355    |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.00698  |\n",
      "| total_timesteps    | 15000    |\n",
      "| value_loss         | 1.78e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.16    |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 0.00436  |\n",
      "| total_timesteps    | 15500    |\n",
      "| value_loss         | 1.02e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.125   |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 0.00305  |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 0.305    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.97    |\n",
      "| fps                | 285      |\n",
      "| nupdates           | 3300     |\n",
      "| policy_entropy     | 0.00451  |\n",
      "| total_timesteps    | 16500    |\n",
      "| value_loss         | 1.22     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.284    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3400     |\n",
      "| policy_entropy     | 0.00371  |\n",
      "| total_timesteps    | 17000    |\n",
      "| value_loss         | 1.66     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0683   |\n",
      "| fps                | 288      |\n",
      "| nupdates           | 3500     |\n",
      "| policy_entropy     | 0.00417  |\n",
      "| total_timesteps    | 17500    |\n",
      "| value_loss         | 3.14     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -11.8    |\n",
      "| fps                | 288      |\n",
      "| nupdates           | 3600     |\n",
      "| policy_entropy     | 0.0107   |\n",
      "| total_timesteps    | 18000    |\n",
      "| value_loss         | 48.2     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -14.2    |\n",
      "| fps                | 290      |\n",
      "| nupdates           | 3700     |\n",
      "| policy_entropy     | 0.0132   |\n",
      "| total_timesteps    | 18500    |\n",
      "| value_loss         | 16.8     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.881   |\n",
      "| fps                | 291      |\n",
      "| nupdates           | 3800     |\n",
      "| policy_entropy     | 0.0674   |\n",
      "| total_timesteps    | 19000    |\n",
      "| value_loss         | 1.17     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.199    |\n",
      "| fps                | 292      |\n",
      "| nupdates           | 3900     |\n",
      "| policy_entropy     | 0.035    |\n",
      "| total_timesteps    | 19500    |\n",
      "| value_loss         | 1.36     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.281   |\n",
      "| fps                | 292      |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.00278  |\n",
      "| total_timesteps    | 20000    |\n",
      "| value_loss         | 0.766    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines.common.policies import LstmPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines import A2C\n",
    "\n",
    "env_1 = gym.make('CartPole-v1')\n",
    "model_1 = A2C(LstmPolicy1, env_1, verbose = 1)\n",
    "model_1.learn(total_timesteps = 20000)\n",
    "model_1.save(\"a2c_cartpole_ver1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d553f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\AppData\\Local\\Temp/ipykernel_20884/3040048196.py:40: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "9.34\n",
      "0.7512656\n"
     ]
    }
   ],
   "source": [
    "eval_env1 = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
    "model_1 = model_1.load('a2c_cartpole_ver1')\n",
    "mean_reward_1, std_reward_1 = evaluate_policy(model_1, eval_env1, n_eval_episodes = 100, return_episode_rewards = False)\n",
    "print(mean_reward_1)\n",
    "print(std_reward_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f30c39",
   "metadata": {},
   "source": [
    "## LSTM Policy 2\n",
    " + 2 Lstm Layers with 128 nodes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfc38759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmPolicy2(RecurrentActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using LSTMs.\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param n_lstm: (int) The number of LSTM cells (for recurrent policies)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param layers: ([int]) The size of the Neural network before the LSTM layer  (if None, default to [64, 64])\n",
    "    :param net_arch: (list) Specification of the actor-critic policy network architecture. Notation similar to the\n",
    "        format described in mlp_extractor but with additional support for a 'lstm' entry in the shared network part.\n",
    "    :param act_fun: (tf.func) the activation function to use in the neural network.\n",
    "    :param cnn_extractor: (function (TensorFlow Tensor, ``**kwargs``): (TensorFlow Tensor)) the CNN feature extraction\n",
    "    :param layer_norm: (bool) Whether or not to use layer normalizing LSTMs\n",
    "    :param feature_extraction: (str) The feature extraction type (\"cnn\" or \"mlp\")\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    recurrent = True\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm=128, reuse=False, layers=None,\n",
    "                 net_arch=None, act_fun=tf.tanh, cnn_extractor=nature_cnn, layer_norm=False, feature_extraction=\"mlp\",\n",
    "                 **kwargs):\n",
    "        # state_shape = [n_lstm * 2] dim because of the cell and hidden states of the LSTM\n",
    "        super(LstmPolicy2, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                                         state_shape=(2 * n_lstm, ), reuse=reuse,\n",
    "                                         scale=(feature_extraction == \"cnn\"))\n",
    "\n",
    "        self._kwargs_check(feature_extraction, kwargs)\n",
    "\n",
    "        if net_arch is None:  # Legacy mode\n",
    "            if layers is None:\n",
    "                layers = [64, 64]\n",
    "            else:\n",
    "                warnings.warn(\"The layers parameter is deprecated. Use the net_arch parameter instead.\")\n",
    "\n",
    "            with tf.variable_scope(\"model\", reuse=reuse):\n",
    "                if feature_extraction == \"cnn\":\n",
    "                    extracted_features = cnn_extractor(self.processed_obs, **kwargs)\n",
    "                else:\n",
    "                    extracted_features = tf.layers.flatten(self.processed_obs)\n",
    "                    for i, layer_size in enumerate(layers):\n",
    "                        extracted_features = act_fun(linear(extracted_features, 'pi_fc' + str(i), n_hidden=layer_size,\n",
    "                                                            init_scale=np.sqrt(2)))\n",
    "                input_sequence = batch_to_seq(extracted_features, self.n_env, n_steps)\n",
    "                masks = batch_to_seq(self.dones_ph, self.n_env, n_steps)\n",
    "                rnn_output, self.snew = lstm(input_sequence, masks, self.states_ph, 'lstm1', n_hidden=n_lstm,\n",
    "                                             layer_norm=layer_norm)\n",
    "                rnn_output, self.snew = lstm(rnn_output, masks, self.snew, 'lstm2', n_hidden=n_lstm,\n",
    "                                             layer_norm=layer_norm)\n",
    "                rnn_output = seq_to_batch(rnn_output)\n",
    "                value_fn = linear(rnn_output, 'vf', 1)\n",
    "\n",
    "                self._proba_distribution, self._policy, self.q_value = \\\n",
    "                    self.pdtype.proba_distribution_from_latent(rnn_output, rnn_output)\n",
    "\n",
    "            self._value_fn = value_fn\n",
    "        else:  # Use the new net_arch parameter\n",
    "            if layers is not None:\n",
    "                warnings.warn(\"The new net_arch parameter overrides the deprecated layers parameter.\")\n",
    "            if feature_extraction == \"cnn\":\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            with tf.variable_scope(\"model\", reuse=reuse):\n",
    "                latent = tf.layers.flatten(self.processed_obs)\n",
    "                policy_only_layers = []  # Layer sizes of the network that only belongs to the policy network\n",
    "                value_only_layers = []  # Layer sizes of the network that only belongs to the value network\n",
    "\n",
    "                # Iterate through the shared layers and build the shared parts of the network\n",
    "                lstm_layer_constructed = False\n",
    "                for idx, layer in enumerate(net_arch):\n",
    "                    if isinstance(layer, int):  # Check that this is a shared layer\n",
    "                        layer_size = layer\n",
    "                        latent = act_fun(linear(latent, \"shared_fc{}\".format(idx), layer_size, init_scale=np.sqrt(2)))\n",
    "                    elif layer == \"lstm\":\n",
    "                        if lstm_layer_constructed:\n",
    "                            raise ValueError(\"The net_arch parameter must only contain one occurrence of 'lstm'!\")\n",
    "                        input_sequence = batch_to_seq(latent, self.n_env, n_steps)\n",
    "                        masks = batch_to_seq(self.dones_ph, self.n_env, n_steps)\n",
    "                        rnn_output, self.snew = lstm(input_sequence, masks, self.states_ph, 'lstm1', n_hidden=n_lstm,\n",
    "                                                     layer_norm=layer_norm)\n",
    "                        rnn_output, self.snew = lstm(rnn_output, masks, self.states_ph, 'lstm2', n_hidden=n_lstm, \n",
    "                                             layer_norm=layer_norm)\n",
    "                        latent = seq_to_batch(rnn_output)\n",
    "                        lstm_layer_constructed = True\n",
    "                    else:\n",
    "                        assert isinstance(layer, dict), \"Error: the net_arch list can only contain ints and dicts\"\n",
    "                        if 'pi' in layer:\n",
    "                            assert isinstance(layer['pi'],\n",
    "                                              list), \"Error: net_arch[-1]['pi'] must contain a list of integers.\"\n",
    "                            policy_only_layers = layer['pi']\n",
    "\n",
    "                        if 'vf' in layer:\n",
    "                            assert isinstance(layer['vf'],\n",
    "                                              list), \"Error: net_arch[-1]['vf'] must contain a list of integers.\"\n",
    "                            value_only_layers = layer['vf']\n",
    "                        break  # From here on the network splits up in policy and value network\n",
    "\n",
    "                # Build the non-shared part of the policy-network\n",
    "                latent_policy = latent\n",
    "                for idx, pi_layer_size in enumerate(policy_only_layers):\n",
    "                    if pi_layer_size == \"lstm\":\n",
    "                        raise NotImplementedError(\"LSTMs are only supported in the shared part of the policy network.\")\n",
    "                    assert isinstance(pi_layer_size, int), \"Error: net_arch[-1]['pi'] must only contain integers.\"\n",
    "                    latent_policy = act_fun(\n",
    "                        linear(latent_policy, \"pi_fc{}\".format(idx), pi_layer_size, init_scale=np.sqrt(2)))\n",
    "\n",
    "                # Build the non-shared part of the value-network\n",
    "                latent_value = latent\n",
    "                for idx, vf_layer_size in enumerate(value_only_layers):\n",
    "                    if vf_layer_size == \"lstm\":\n",
    "                        raise NotImplementedError(\"LSTMs are only supported in the shared part of the value function \"\n",
    "                                                  \"network.\")\n",
    "                    assert isinstance(vf_layer_size, int), \"Error: net_arch[-1]['vf'] must only contain integers.\"\n",
    "                    latent_value = act_fun(\n",
    "                        linear(latent_value, \"vf_fc{}\".format(idx), vf_layer_size, init_scale=np.sqrt(2)))\n",
    "\n",
    "                if not lstm_layer_constructed:\n",
    "                    raise ValueError(\"The net_arch parameter must contain at least one occurrence of 'lstm'!\")\n",
    "\n",
    "                self._value_fn = linear(latent_value, 'vf', 1)\n",
    "                # TODO: why not init_scale = 0.001 here like in the feedforward\n",
    "                self._proba_distribution, self._policy, self.q_value = \\\n",
    "                    self.pdtype.proba_distribution_from_latent(latent_policy, latent_value)\n",
    "        self._setup_init()\n",
    "\n",
    "    def step(self, obs, state=None, mask=None, deterministic=False):\n",
    "        if deterministic:\n",
    "            return self.sess.run([self.deterministic_action, self.value_flat, self.snew, self.neglogp],\n",
    "                                 {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})\n",
    "        else:\n",
    "            return self.sess.run([self.action, self.value_flat, self.snew, self.neglogp],\n",
    "                                 {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})\n",
    "\n",
    "    def proba_step(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.policy_proba, {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})\n",
    "\n",
    "    def value(self, obs, state=None, mask=None):\n",
    "        return self.sess.run(self.value_flat, {self.obs_ph: obs, self.states_ph: state, self.dones_ph: mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99332ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| explained_variance | 0.01     |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.00771  |\n",
      "| fps                | 201      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 500      |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0246  |\n",
      "| fps                | 238      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 10.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0259   |\n",
      "| fps                | 254      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 3.39     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00533 |\n",
      "| fps                | 268      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 10.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.026   |\n",
      "| fps                | 276      |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 3.02     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0202  |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 3000     |\n",
      "| value_loss         | 5.95     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.018   |\n",
      "| fps                | 282      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 10.3     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0927   |\n",
      "| fps                | 282      |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 0.692    |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 9.73     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0694  |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.689    |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 10       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.61    |\n",
      "| fps                | 282      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.692    |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 39.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.127   |\n",
      "| fps                | 282      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.65     |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 6.19     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -4.42    |\n",
      "| fps                | 282      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.686    |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 7.27     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0174  |\n",
      "| fps                | 282      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 0.653    |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 3.41     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0603  |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 0.599    |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 3        |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.24    |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 0.687    |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 2.53     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.107   |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 0.69     |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 1.91     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -597     |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.564    |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 316      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.211   |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 0.683    |\n",
      "| total_timesteps    | 9000     |\n",
      "| value_loss         | 1.12     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0389  |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 0.683    |\n",
      "| total_timesteps    | 9500     |\n",
      "| value_loss         | 0.841    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0537  |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.691    |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 0.5      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -50.8    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 0.277    |\n",
      "| total_timesteps    | 10500    |\n",
      "| value_loss         | 0.456    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -3.36e+03 |\n",
      "| fps                | 280       |\n",
      "| nupdates           | 2200      |\n",
      "| policy_entropy     | 0.402     |\n",
      "| total_timesteps    | 11000     |\n",
      "| value_loss         | 197       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -1.06e+04 |\n",
      "| fps                | 280       |\n",
      "| nupdates           | 2300      |\n",
      "| policy_entropy     | 0.152     |\n",
      "| total_timesteps    | 11500     |\n",
      "| value_loss         | 197       |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.348   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2400     |\n",
      "| policy_entropy     | 0.122    |\n",
      "| total_timesteps    | 12000    |\n",
      "| value_loss         | 186      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.45    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2500     |\n",
      "| policy_entropy     | 0.0951   |\n",
      "| total_timesteps    | 12500    |\n",
      "| value_loss         | 179      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0244   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2600     |\n",
      "| policy_entropy     | 0.686    |\n",
      "| total_timesteps    | 13000    |\n",
      "| value_loss         | 0.00199  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -399     |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2700     |\n",
      "| policy_entropy     | 0.0846   |\n",
      "| total_timesteps    | 13500    |\n",
      "| value_loss         | 0.0135   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.259   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 0.0236   |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 143      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.263   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 0.00933  |\n",
      "| total_timesteps    | 14500    |\n",
      "| value_loss         | 94.3     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | -0.253   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.00469  |\n",
      "| total_timesteps    | 15000    |\n",
      "| value_loss         | 40.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.262    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 0.00383  |\n",
      "| total_timesteps    | 15500    |\n",
      "| value_loss         | 226      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0809   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 0.00946  |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 235      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.6     |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3300     |\n",
      "| policy_entropy     | 0.000211 |\n",
      "| total_timesteps    | 16500    |\n",
      "| value_loss         | 0.124    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.423   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3400     |\n",
      "| policy_entropy     | 0.000386 |\n",
      "| total_timesteps    | 17000    |\n",
      "| value_loss         | 7.24e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.259   |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3500     |\n",
      "| policy_entropy     | 0.00292  |\n",
      "| total_timesteps    | 17500    |\n",
      "| value_loss         | 58.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -6.56    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3600     |\n",
      "| policy_entropy     | 0.00232  |\n",
      "| total_timesteps    | 18000    |\n",
      "| value_loss         | 0.0497   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -79.9    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3700     |\n",
      "| policy_entropy     | 0.00877  |\n",
      "| total_timesteps    | 18500    |\n",
      "| value_loss         | 164      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -220     |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3800     |\n",
      "| policy_entropy     | 0.0231   |\n",
      "| total_timesteps    | 19000    |\n",
      "| value_loss         | 1.44e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.176    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 3900     |\n",
      "| policy_entropy     | 0.0342   |\n",
      "| total_timesteps    | 19500    |\n",
      "| value_loss         | 343      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.944    |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.00117  |\n",
      "| total_timesteps    | 20000    |\n",
      "| value_loss         | 64.5     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env_2 = gym.make('CartPole-v1')\n",
    "model_2 = A2C(LstmPolicy2, env_2, verbose = 1)\n",
    "model_2.learn(total_timesteps = 20000)\n",
    "model_2.save(\"a2c_cartpole_ver2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d4aee35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\AppData\\Local\\Temp/ipykernel_20884/3172821659.py:40: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "9.31 0.77064914\n"
     ]
    }
   ],
   "source": [
    "eval_env2 = DummyVecEnv([lambda:gym.make('CartPole-v1')])\n",
    "model_2 = model_2.load(\"a2c_cartpole_ver2\")\n",
    "mean_reward_2, std_reward_2 = evaluate_policy(model_2, eval_env2, n_eval_episodes=100, return_episode_rewards=False)\n",
    "print(mean_reward_2, std_reward_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2f585",
   "metadata": {},
   "source": [
    "## LSTM Policy 3\n",
    " + 2 Lstm layer (128 nodes each) + 2 MLP layers for actor and critic layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa6d4225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| explained_variance | 0.00179  |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 9.06e-06 |\n",
      "| fps                | 198      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 500      |\n",
      "| value_loss         | 10.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.000252 |\n",
      "| fps                | 240      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0226  |\n",
      "| fps                | 255      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 5.94     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.00138  |\n",
      "| fps                | 263      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 10.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00295 |\n",
      "| fps                | 268      |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 10.3     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.133   |\n",
      "| fps                | 272      |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 3000     |\n",
      "| value_loss         | 10.9     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.069   |\n",
      "| fps                | 275      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 0.692    |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 10.1     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.14    |\n",
      "| fps                | 277      |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 0.692    |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 15.8     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.253   |\n",
      "| fps                | 279      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.672    |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 9.64     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0158  |\n",
      "| fps                | 280      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.685    |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 7.43     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0517  |\n",
      "| fps                | 281      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.69     |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 6.87     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0488  |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.61     |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 6.25     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0195  |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 0.518    |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 5.74     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00857 |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 0.609    |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 655      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0303  |\n",
      "| fps                | 284      |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 0.691    |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 182      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0395  |\n",
      "| fps                | 285      |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 0.683    |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 582      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0214  |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.624    |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 4.52     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00454 |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 0.505    |\n",
      "| total_timesteps    | 9000     |\n",
      "| value_loss         | 4.25     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00126 |\n",
      "| fps                | 288      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 0.6      |\n",
      "| total_timesteps    | 9500     |\n",
      "| value_loss         | 4.04     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.129   |\n",
      "| fps                | 288      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.691    |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 4.04     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.123   |\n",
      "| fps                | 289      |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 0.688    |\n",
      "| total_timesteps    | 10500    |\n",
      "| value_loss         | 3.74     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00284 |\n",
      "| fps                | 291      |\n",
      "| nupdates           | 2200     |\n",
      "| policy_entropy     | 0.678    |\n",
      "| total_timesteps    | 11000    |\n",
      "| value_loss         | 3.22     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0103  |\n",
      "| fps                | 293      |\n",
      "| nupdates           | 2300     |\n",
      "| policy_entropy     | 0.554    |\n",
      "| total_timesteps    | 11500    |\n",
      "| value_loss         | 833      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00719 |\n",
      "| fps                | 293      |\n",
      "| nupdates           | 2400     |\n",
      "| policy_entropy     | 0.53     |\n",
      "| total_timesteps    | 12000    |\n",
      "| value_loss         | 440      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00811 |\n",
      "| fps                | 293      |\n",
      "| nupdates           | 2500     |\n",
      "| policy_entropy     | 0.69     |\n",
      "| total_timesteps    | 12500    |\n",
      "| value_loss         | 911      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -0.000396 |\n",
      "| fps                | 293       |\n",
      "| nupdates           | 2600      |\n",
      "| policy_entropy     | 0.552     |\n",
      "| total_timesteps    | 13000     |\n",
      "| value_loss         | 2.49      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0083  |\n",
      "| fps                | 295      |\n",
      "| nupdates           | 2700     |\n",
      "| policy_entropy     | 0.333    |\n",
      "| total_timesteps    | 13500    |\n",
      "| value_loss         | 2.38     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00622 |\n",
      "| fps                | 296      |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 0.443    |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 1.1e+03  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00264 |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 0.57     |\n",
      "| total_timesteps    | 14500    |\n",
      "| value_loss         | 2.19     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | -0.00166 |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.549    |\n",
      "| total_timesteps    | 15000    |\n",
      "| value_loss         | 2.05     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0833  |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 0.687    |\n",
      "| total_timesteps    | 15500    |\n",
      "| value_loss         | 1.91     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0134  |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 0.558    |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 1.65     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0268  |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 3300     |\n",
      "| policy_entropy     | 0.488    |\n",
      "| total_timesteps    | 16500    |\n",
      "| value_loss         | 1.6      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.364   |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 3400     |\n",
      "| policy_entropy     | 0.665    |\n",
      "| total_timesteps    | 17000    |\n",
      "| value_loss         | 1.78     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00229 |\n",
      "| fps                | 297      |\n",
      "| nupdates           | 3500     |\n",
      "| policy_entropy     | 0.654    |\n",
      "| total_timesteps    | 17500    |\n",
      "| value_loss         | 3.83e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -3.77e-05 |\n",
      "| fps                | 297       |\n",
      "| nupdates           | 3600      |\n",
      "| policy_entropy     | 0.693     |\n",
      "| total_timesteps    | 18000     |\n",
      "| value_loss         | 1.17      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -4.52e-05 |\n",
      "| fps                | 297       |\n",
      "| nupdates           | 3700      |\n",
      "| policy_entropy     | 0.68      |\n",
      "| total_timesteps    | 18500     |\n",
      "| value_loss         | 1.03      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00456 |\n",
      "| fps                | 298      |\n",
      "| nupdates           | 3800     |\n",
      "| policy_entropy     | 0.595    |\n",
      "| total_timesteps    | 19000    |\n",
      "| value_loss         | 2.85e+03 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00721 |\n",
      "| fps                | 298      |\n",
      "| nupdates           | 3900     |\n",
      "| policy_entropy     | 0.662    |\n",
      "| total_timesteps    | 19500    |\n",
      "| value_loss         | 0.82     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -0.000737 |\n",
      "| fps                | 298       |\n",
      "| nupdates           | 4000      |\n",
      "| policy_entropy     | 0.549     |\n",
      "| total_timesteps    | 20000     |\n",
      "| value_loss         | 0.714     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model_3 = A2C(LstmPolicy2, env, verbose = 1, policy_kwargs = dict(net_arch = ['lstm', dict(pi = [64, 64], vf = [64, 64])]))\n",
    "model_3.learn(total_timesteps = 20000)\n",
    "model_3.save('a2c_cartpole_ver3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd24b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n",
      "WARNING:tensorflow:From C:\\Users\\hyewon\\AppData\\Local\\Temp/ipykernel_20884/3172821659.py:67: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "[<tf.Variable 'model/lstm1/wx:0' shape=(4, 512) dtype=float32_ref>, <tf.Variable 'model/lstm1/wh:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'model/lstm1/b:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'model/lstm2/wx:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'model/lstm2/wh:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'model/lstm2/b:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'model/pi_fc0/w:0' shape=(128, 64) dtype=float32_ref>, <tf.Variable 'model/pi_fc0/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'model/pi_fc1/w:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'model/pi_fc1/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'model/vf_fc0/w:0' shape=(128, 64) dtype=float32_ref>, <tf.Variable 'model/vf_fc0/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'model/vf_fc1/w:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'model/vf_fc1/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'model/vf/w:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'model/vf/b:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'model/pi/w:0' shape=(64, 2) dtype=float32_ref>, <tf.Variable 'model/pi/b:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'model/q/w:0' shape=(64, 2) dtype=float32_ref>, <tf.Variable 'model/q/b:0' shape=(2,) dtype=float32_ref>]\n",
      "\n",
      "\n",
      "9.43 0.6964912\n"
     ]
    }
   ],
   "source": [
    "eval_env3 = DummyVecEnv([lambda:gym.make('CartPole-v1')])\n",
    "model_3 = model_3.load(\"a2c_cartpole_ver3\")\n",
    "print(model_3.get_parameter_list())\n",
    "mean_reward_3, std_reward_3 = evaluate_policy(model_3, eval_env3, n_eval_episodes=100, return_episode_rewards=False)\n",
    "print('\\n')\n",
    "print(mean_reward_3, std_reward_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1b7ab",
   "metadata": {},
   "source": [
    "## Cutom Lstm Policy based MlpLstmPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b95f5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpLstmPolicy2(LstmPolicy2):\n",
    "    \"\"\"\n",
    "    Policy object that implements actor critic, using LSTMs with a MLP feature extraction\n",
    "    :param sess: (TensorFlow session) The current TensorFlow session\n",
    "    :param ob_space: (Gym Space) The observation space of the environment\n",
    "    :param ac_space: (Gym Space) The action space of the environment\n",
    "    :param n_env: (int) The number of environments to run\n",
    "    :param n_steps: (int) The number of steps to run for each environment\n",
    "    :param n_batch: (int) The number of batch to run (n_envs * n_steps)\n",
    "    :param n_lstm: (int) The number of LSTM cells (for recurrent policies)\n",
    "    :param reuse: (bool) If the policy is reusable or not\n",
    "    :param kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm = 128, reuse = False, **_kwargs):\n",
    "        super(MlpLstmPolicy2, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                                            layer_norm=False, feature_extraction = \"mlp\", **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79cf281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| explained_variance | -0.00054 |\n",
      "| fps                | 5        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 10.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00265 |\n",
      "| fps                | 192      |\n",
      "| nupdates           | 100      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 500      |\n",
      "| value_loss         | 6.04     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.000757 |\n",
      "| fps                | 229      |\n",
      "| nupdates           | 200      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1000     |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.000269 |\n",
      "| fps                | 244      |\n",
      "| nupdates           | 300      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 1500     |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0124  |\n",
      "| fps                | 261      |\n",
      "| nupdates           | 400      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2000     |\n",
      "| value_loss         | 10.7     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00278 |\n",
      "| fps                | 268      |\n",
      "| nupdates           | 500      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 2500     |\n",
      "| value_loss         | 10.6     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00181 |\n",
      "| fps                | 271      |\n",
      "| nupdates           | 600      |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 3000     |\n",
      "| value_loss         | 10.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00533 |\n",
      "| fps                | 277      |\n",
      "| nupdates           | 700      |\n",
      "| policy_entropy     | 0.689    |\n",
      "| total_timesteps    | 3500     |\n",
      "| value_loss         | 6.54     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.00248  |\n",
      "| fps                | 279      |\n",
      "| nupdates           | 800      |\n",
      "| policy_entropy     | 0.685    |\n",
      "| total_timesteps    | 4000     |\n",
      "| value_loss         | 10.2     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.14    |\n",
      "| fps                | 283      |\n",
      "| nupdates           | 900      |\n",
      "| policy_entropy     | 0.685    |\n",
      "| total_timesteps    | 4500     |\n",
      "| value_loss         | 16.9     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.09    |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1000     |\n",
      "| policy_entropy     | 0.691    |\n",
      "| total_timesteps    | 5000     |\n",
      "| value_loss         | 36.1     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0314  |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1100     |\n",
      "| policy_entropy     | 0.659    |\n",
      "| total_timesteps    | 5500     |\n",
      "| value_loss         | 5.89     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00686 |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 1200     |\n",
      "| policy_entropy     | 0.689    |\n",
      "| total_timesteps    | 6000     |\n",
      "| value_loss         | 846      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0301  |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 1300     |\n",
      "| policy_entropy     | 0.692    |\n",
      "| total_timesteps    | 6500     |\n",
      "| value_loss         | 3.28     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -31.4    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 1400     |\n",
      "| policy_entropy     | 0.614    |\n",
      "| total_timesteps    | 7000     |\n",
      "| value_loss         | 6.66     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -3.89    |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1500     |\n",
      "| policy_entropy     | 0.264    |\n",
      "| total_timesteps    | 7500     |\n",
      "| value_loss         | 3.79     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -343     |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1600     |\n",
      "| policy_entropy     | 0.664    |\n",
      "| total_timesteps    | 8000     |\n",
      "| value_loss         | 277      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -4.64    |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1700     |\n",
      "| policy_entropy     | 0.425    |\n",
      "| total_timesteps    | 8500     |\n",
      "| value_loss         | 2.1      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0517  |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1800     |\n",
      "| policy_entropy     | 0.691    |\n",
      "| total_timesteps    | 9000     |\n",
      "| value_loss         | 1.02     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.846   |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 1900     |\n",
      "| policy_entropy     | 0.189    |\n",
      "| total_timesteps    | 9500     |\n",
      "| value_loss         | 461      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.03    |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 2000     |\n",
      "| policy_entropy     | 0.146    |\n",
      "| total_timesteps    | 10000    |\n",
      "| value_loss         | 238      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -2.77    |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 2100     |\n",
      "| policy_entropy     | 0.382    |\n",
      "| total_timesteps    | 10500    |\n",
      "| value_loss         | 0.327    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.101   |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 2200     |\n",
      "| policy_entropy     | 0.69     |\n",
      "| total_timesteps    | 11000    |\n",
      "| value_loss         | 0.141    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.292   |\n",
      "| fps                | 285      |\n",
      "| nupdates           | 2300     |\n",
      "| policy_entropy     | 0.0555   |\n",
      "| total_timesteps    | 11500    |\n",
      "| value_loss         | 151      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -5.62e+04 |\n",
      "| fps                | 285       |\n",
      "| nupdates           | 2400      |\n",
      "| policy_entropy     | 0.0203    |\n",
      "| total_timesteps    | 12000     |\n",
      "| value_loss         | 136       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| explained_variance | -1.67e+03 |\n",
      "| fps                | 285       |\n",
      "| nupdates           | 2500      |\n",
      "| policy_entropy     | 0.00512   |\n",
      "| total_timesteps    | 12500     |\n",
      "| value_loss         | 0.011     |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -275     |\n",
      "| fps                | 285      |\n",
      "| nupdates           | 2600     |\n",
      "| policy_entropy     | 0.00878  |\n",
      "| total_timesteps    | 13000    |\n",
      "| value_loss         | 63       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -23.7    |\n",
      "| fps                | 285      |\n",
      "| nupdates           | 2700     |\n",
      "| policy_entropy     | 0.00331  |\n",
      "| total_timesteps    | 13500    |\n",
      "| value_loss         | 0.298    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.716    |\n",
      "| fps                | 286      |\n",
      "| nupdates           | 2800     |\n",
      "| policy_entropy     | 0.00383  |\n",
      "| total_timesteps    | 14000    |\n",
      "| value_loss         | 343      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -68.5    |\n",
      "| fps                | 288      |\n",
      "| nupdates           | 2900     |\n",
      "| policy_entropy     | 0.00895  |\n",
      "| total_timesteps    | 14500    |\n",
      "| value_loss         | 563      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| explained_variance | -31.3    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3000     |\n",
      "| policy_entropy     | 0.00774  |\n",
      "| total_timesteps    | 15000    |\n",
      "| value_loss         | 293      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -9.47    |\n",
      "| fps                | 288      |\n",
      "| nupdates           | 3100     |\n",
      "| policy_entropy     | 8.01e-05 |\n",
      "| total_timesteps    | 15500    |\n",
      "| value_loss         | 0.338    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.494   |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3200     |\n",
      "| policy_entropy     | 0.00211  |\n",
      "| total_timesteps    | 16000    |\n",
      "| value_loss         | 5.73     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -45      |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3300     |\n",
      "| policy_entropy     | 0.00168  |\n",
      "| total_timesteps    | 16500    |\n",
      "| value_loss         | 34.5     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -85.5    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3400     |\n",
      "| policy_entropy     | 4.23e-05 |\n",
      "| total_timesteps    | 17000    |\n",
      "| value_loss         | 0.205    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -43.7    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3500     |\n",
      "| policy_entropy     | 3.47e-05 |\n",
      "| total_timesteps    | 17500    |\n",
      "| value_loss         | 0.721    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.162   |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3600     |\n",
      "| policy_entropy     | 0.00146  |\n",
      "| total_timesteps    | 18000    |\n",
      "| value_loss         | 2.97     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -3.87    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3700     |\n",
      "| policy_entropy     | 3.15e-05 |\n",
      "| total_timesteps    | 18500    |\n",
      "| value_loss         | 0.584    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0208  |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3800     |\n",
      "| policy_entropy     | 0.00191  |\n",
      "| total_timesteps    | 19000    |\n",
      "| value_loss         | 0.242    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.00845 |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 3900     |\n",
      "| policy_entropy     | 0.00361  |\n",
      "| total_timesteps    | 19500    |\n",
      "| value_loss         | 0.368    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -1.88    |\n",
      "| fps                | 287      |\n",
      "| nupdates           | 4000     |\n",
      "| policy_entropy     | 0.006    |\n",
      "| total_timesteps    | 20000    |\n",
      "| value_loss         | 0.124    |\n",
      "---------------------------------\n",
      "[<tf.Variable 'model/pi_fc0/w:0' shape=(4, 64) dtype=float32_ref>, <tf.Variable 'model/pi_fc0/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'model/pi_fc1/w:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'model/pi_fc1/b:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'model/lstm1/wx:0' shape=(64, 512) dtype=float32_ref>, <tf.Variable 'model/lstm1/wh:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'model/lstm1/b:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'model/lstm2/wx:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'model/lstm2/wh:0' shape=(128, 512) dtype=float32_ref>, <tf.Variable 'model/lstm2/b:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'model/vf/w:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'model/vf/b:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'model/pi/w:0' shape=(128, 2) dtype=float32_ref>, <tf.Variable 'model/pi/b:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'model/q/w:0' shape=(128, 2) dtype=float32_ref>, <tf.Variable 'model/q/b:0' shape=(2,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = A2C(MlpLstmPolicy2, env, verbose = 1)\n",
    "model.learn(total_timesteps = 20000)\n",
    "params = model.get_parameter_list()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6664eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "11.57 0.8631917\n"
     ]
    }
   ],
   "source": [
    "eval_env = DummyVecEnv([lambda:gym.make('CartPole-v1')])\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100, return_episode_rewards=False)\n",
    "print('\\n')\n",
    "print(mean_reward, std_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
